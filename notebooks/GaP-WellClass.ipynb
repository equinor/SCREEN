{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from matplotlib import font_manager\n",
    "%matplotlib inline\n",
    "import matplotlib.colors as colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resource configuration\n",
    "rcParams['figure.dpi'] = 200\n",
    "\n",
    "# fount information\n",
    "rcParams['font.family'] = 'Equinor'\n",
    "rcParams['font.size'] = 9\n",
    "\n",
    "# fornt location\n",
    "font_path = r'Equinor_regular'\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_path)\n",
    "\n",
    "for font_file in font_files:\n",
    "    font_manager.fontManager.addfont(font_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecl.eclfile import EclFile\n",
    "from ecl.eclfile import EclInitFile, EclRestartFile\n",
    "from ecl.grid import EclGrid\n",
    "#import rips "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.17\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where WellClass and Ga[ codes are located\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'type' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# GaP\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mGaP\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlibs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     PipeCementModel,\n\u001b[1;32m      4\u001b[0m     ElemModel,\n\u001b[1;32m      5\u001b[0m     DepthModel,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mGaP\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlibs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcarfin\u001b[39;00m \u001b[39mimport\u001b[39;00m build_grdecl\n",
      "File \u001b[0;32m/scratch/SCS/hzh/SCREEN/notebooks/../src/GaP/libs/models/__init__.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodel_depth\u001b[39;00m \u001b[39mimport\u001b[39;00m DepthModel\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodel_file\u001b[39;00m \u001b[39mimport\u001b[39;00m FileModel\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodel_gap\u001b[39;00m \u001b[39mimport\u001b[39;00m GaPModel\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodel_pipe_cement\u001b[39;00m \u001b[39mimport\u001b[39;00m PipeCementModel, ElemModel\n",
      "File \u001b[0;32m/scratch/SCS/hzh/SCREEN/notebooks/../src/GaP/libs/models/model_gap.py:7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m annotations\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydantic\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseModel\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodel_pipe_cement\u001b[39;00m \u001b[39mimport\u001b[39;00m PipeCementModel, ElemModel\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodel_defaults\u001b[39;00m \u001b[39mimport\u001b[39;00m DefaultModel\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodel_file\u001b[39;00m \u001b[39mimport\u001b[39;00m FileModel\n",
      "File \u001b[0;32m/scratch/SCS/hzh/SCREEN/notebooks/../src/GaP/libs/models/model_pipe_cement.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydantic\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseModel\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodel_depth\u001b[39;00m \u001b[39mimport\u001b[39;00m DepthModel\n\u001b[0;32m---> 11\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mElemModel\u001b[39;00m(BaseModel):\n\u001b[1;32m     12\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" define base model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     ID: \u001b[39mfloat\u001b[39m\n",
      "File \u001b[0;32m/scratch/SCS/hzh/apps/mambaforge/envs/venv_screen_py3.8/lib/python3.8/site-packages/pydantic/_internal/_model_construction.py:177\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[0;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m     parent_namespace \u001b[39m=\u001b[39m unpack_lenient_weakvaluedict(parent_namespace)\n\u001b[1;32m    176\u001b[0m types_namespace \u001b[39m=\u001b[39m get_cls_types_namespace(\u001b[39mcls\u001b[39m, parent_namespace)\n\u001b[0;32m--> 177\u001b[0m set_model_fields(\u001b[39mcls\u001b[39;49m, bases, config_wrapper, types_namespace)\n\u001b[1;32m    178\u001b[0m complete_model_class(\n\u001b[1;32m    179\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m    180\u001b[0m     cls_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m     types_namespace\u001b[39m=\u001b[39mtypes_namespace,\n\u001b[1;32m    184\u001b[0m )\n\u001b[1;32m    185\u001b[0m \u001b[39m# using super(cls, cls) on the next line ensures we only call the parent class's __pydantic_init_subclass__\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m# I believe the `type: ignore` is only necessary because mypy doesn't realize that this code branch is\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m# only hit for _proper_ subclasses of BaseModel\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/SCS/hzh/apps/mambaforge/envs/venv_screen_py3.8/lib/python3.8/site-packages/pydantic/_internal/_model_construction.py:405\u001b[0m, in \u001b[0;36mset_model_fields\u001b[0;34m(cls, bases, config_wrapper, types_namespace)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Collect and set `cls.model_fields` and `cls.__class_vars__`.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \n\u001b[1;32m    398\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39m    types_namespace: Optional extra namespace to look for types in.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    404\u001b[0m typevars_map \u001b[39m=\u001b[39m get_model_typevars_map(\u001b[39mcls\u001b[39m)\n\u001b[0;32m--> 405\u001b[0m fields, class_vars \u001b[39m=\u001b[39m collect_model_fields(\u001b[39mcls\u001b[39;49m, bases, config_wrapper, types_namespace, typevars_map\u001b[39m=\u001b[39;49mtypevars_map)\n\u001b[1;32m    407\u001b[0m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_fields \u001b[39m=\u001b[39m fields\n\u001b[1;32m    408\u001b[0m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__class_vars__\u001b[39m.\u001b[39mupdate(class_vars)\n",
      "File \u001b[0;32m/scratch/SCS/hzh/apps/mambaforge/envs/venv_screen_py3.8/lib/python3.8/site-packages/pydantic/_internal/_fields.py:98\u001b[0m, in \u001b[0;36mcollect_model_fields\u001b[0;34m(cls, bases, config_wrapper, types_namespace, typevars_map)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Collect the fields of a nascent pydantic model.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \n\u001b[1;32m     76\u001b[0m \u001b[39mAlso collect the names of any ClassVars present in the type hints.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m        - If a field shadows an attribute in the parent model.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfields\u001b[39;00m \u001b[39mimport\u001b[39;00m FieldInfo\n\u001b[0;32m---> 98\u001b[0m type_hints \u001b[39m=\u001b[39m get_cls_type_hints_lenient(\u001b[39mcls\u001b[39;49m, types_namespace)\n\u001b[1;32m    100\u001b[0m \u001b[39m# https://docs.python.org/3/howto/annotations.html#accessing-the-annotations-dict-of-an-object-in-python-3-9-and-older\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m# annotations is only used for finding fields in parent classes\u001b[39;00m\n\u001b[1;32m    102\u001b[0m annotations \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m__annotations__\u001b[39m\u001b[39m'\u001b[39m, {})\n",
      "File \u001b[0;32m/scratch/SCS/hzh/apps/mambaforge/envs/venv_screen_py3.8/lib/python3.8/site-packages/pydantic/_internal/_typing_extra.py:212\u001b[0m, in \u001b[0;36mget_cls_type_hints_lenient\u001b[0;34m(obj, globalns)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[39mif\u001b[39;00m ann \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m ann \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m GetSetDescriptorType:\n\u001b[1;32m    211\u001b[0m         \u001b[39mfor\u001b[39;00m name, value \u001b[39min\u001b[39;00m ann\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 212\u001b[0m             hints[name] \u001b[39m=\u001b[39m eval_type_lenient(value, globalns, localns)\n\u001b[1;32m    213\u001b[0m \u001b[39mreturn\u001b[39;00m hints\n",
      "File \u001b[0;32m/scratch/SCS/hzh/apps/mambaforge/envs/venv_screen_py3.8/lib/python3.8/site-packages/pydantic/_internal/_typing_extra.py:224\u001b[0m, in \u001b[0;36meval_type_lenient\u001b[0;34m(value, globalns, localns)\u001b[0m\n\u001b[1;32m    221\u001b[0m     value \u001b[39m=\u001b[39m _make_forward_ref(value, is_argument\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, is_class\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    223\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m typing\u001b[39m.\u001b[39;49m_eval_type(value, globalns, localns)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNameError\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     \u001b[39m# the point of this function is to be tolerant to this case\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m/scratch/SCS/hzh/apps/mambaforge/envs/venv_screen_py3.8/lib/python3.8/typing.py:270\u001b[0m, in \u001b[0;36m_eval_type\u001b[0;34m(t, globalns, localns)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Evaluate all forward references in the given type t.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[39mFor use of globalns and localns see the docstring for get_type_hints().\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, ForwardRef):\n\u001b[0;32m--> 270\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49m_evaluate(globalns, localns)\n\u001b[1;32m    271\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, _GenericAlias):\n\u001b[1;32m    272\u001b[0m     ev_args \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(_eval_type(a, globalns, localns) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39m__args__)\n",
      "File \u001b[0;32m/scratch/SCS/hzh/apps/mambaforge/envs/venv_screen_py3.8/lib/python3.8/typing.py:518\u001b[0m, in \u001b[0;36mForwardRef._evaluate\u001b[0;34m(self, globalns, localns)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[39melif\u001b[39;00m localns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    516\u001b[0m         localns \u001b[39m=\u001b[39m globalns\n\u001b[1;32m    517\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__forward_value__ \u001b[39m=\u001b[39m _type_check(\n\u001b[0;32m--> 518\u001b[0m         \u001b[39meval\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__forward_code__, globalns, localns),\n\u001b[1;32m    519\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mForward references must evaluate to types.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    520\u001b[0m         is_argument\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__forward_is_argument__)\n\u001b[1;32m    521\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__forward_evaluated__ \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__forward_value__\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'type' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# GaP\n",
    "from src.GaP.libs.models import (\n",
    "    PipeCementModel,\n",
    "    ElemModel,\n",
    "    DepthModel,\n",
    ")\n",
    "\n",
    "from src.GaP.libs.carfin import build_grdecl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WellClass\n",
    "from src.WellClass.libs.well_class import Well\n",
    "\n",
    "from src.WellClass.libs.utils import (\n",
    "    csv_parser,\n",
    "    yaml_parser,\n",
    ")\n",
    "from src.WellClass.libs.grid_utils.LGR_grid_utils import (\n",
    "    compute_ngrd,\n",
    "    generate_LGR_xy,\n",
    "    generate_LGR_z,\n",
    ")\n",
    "from src.WellClass.libs.grid_utils.LGR_bbox import (\n",
    "    get_ij_indices,\n",
    "    get_k_indices,\n",
    ")\n",
    "from src.WellClass.libs.grid_utils.df2gap import (\n",
    "    to_gap_casing_list,\n",
    "    to_gap_barrier_list\n",
    ")\n",
    "\n",
    "\n",
    "# plots\n",
    "from src.WellClass.libs.plotting import plot_well_perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some user options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(hzh): use Ali's algorithm\n",
    "Ali_way = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use yaml or csv input file\n",
    "use_yaml = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick an example from given three options\n",
    "\n",
    "# case_type = 'cosmo'\n",
    "\n",
    "case_type = 'smeaheia_v1'\n",
    "\n",
    "# case_type = 'smeaheia_v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "The following are the test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples\n",
    "smeaheia_v1 = {'well_input': r'GaP_input_Smeaheia_v3.csv', \n",
    "               'well_input_yaml': r'smeaheia.yaml', \n",
    "            #    'sim_path': r'/scratch/SCS/eim/SMEAHEIA', \n",
    "               'sim_path': r'../test_data/examples/smeaheia_v1',\n",
    "               'simcase': r'GEN_NOLGR_PH2'}\n",
    "smeaheia_v2 = {'well_input': r'GaP_input_Smeaheia_v3.csv', \n",
    "               'well_input_yaml': r'smeaheia.yaml', \n",
    "            #    'sim_path': r'/scratch/SCS/bkh/wbook/realization-0/iter-0/pflotran/model', \n",
    "               'sim_path': r'../test_data/examples/smeaheia_v2', \n",
    "               'simcase': r'TEMP-0'}\n",
    "cosmo = {\n",
    "         'well_input': r'GaP_input_Cosmo_v3.csv', \n",
    "         'well_input_yaml': r'cosmo.yaml', \n",
    "        #  'sim_path': r'/scratch/SCS/bkh/well_class_test1/realization-0/iter-0/pflotran/model', \n",
    "         'sim_path': r'../test_data/examples/cosmo', \n",
    "         'simcase': r'TEMP-0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = {\n",
    "    'smeaheia_v1': smeaheia_v1,\n",
    "    'smeaheia_v2': smeaheia_v2,\n",
    "    'cosmo': cosmo\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load well CSV or yaml configuration file\n",
    "\n",
    "Process CSV with well class.\n",
    "Predefine a dictionary that includes the input CSV well file, the simulation path, and the PFT sim case name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the selected example for testing\n",
    "case = examples[case_type]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# root_path = '/scratch/SCS/gpb/SCREEN/GaP_code'\n",
    "\n",
    "# where the location for the input parameters and eclipse .EGRID and .INIT files\n",
    "sim_path = case['sim_path']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_yaml:\n",
    "    # where well configuration file is located\n",
    "    well_name = os.path.join(sim_path, case['well_input_yaml'])\n",
    "    \n",
    "    # # pydantic model\n",
    "    well_model = yaml_parser(well_name)\n",
    "    well_csv = json.loads(well_model.spec.model_dump_json())\n",
    "else:\n",
    "    # where well configuration file is located\n",
    "    well_name = os.path.join(sim_path, case['well_input'])\n",
    "\n",
    "    # load the well information\n",
    "    well_csv = csv_parser(well_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Process well by running well class\n",
    "my_well = Well( header       = well_csv['well_header'], \n",
    "                drilling     = well_csv['drilling'],\n",
    "                casings      = well_csv['casing_cement'],\n",
    "                geology      = well_csv['geology'],\n",
    "                barriers     = well_csv['barriers'], \n",
    "                barrier_perm = well_csv['barrier_permeability'],\n",
    "                co2_datum    = well_csv['co2_datum'],\n",
    "           )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model\n",
    "\n",
    "- Load the PFT grid, init and restart files.\n",
    "- Grid contains geometry specs\n",
    "- INIT contains static properties (i.e. poro., perm., transmissibilities)\n",
    "- RST contains dynamic properties (i.e. saturations, pressure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation case without legacy well \n",
    "\n",
    "# path = '/scratch/SCS/bkh/wbook/realization-0/iter-0/pflotran/model'\n",
    "\n",
    "# location of .egrid\n",
    "simcase = os.path.join(sim_path, case['simcase'])\n",
    "\n",
    "#Get grid dimensions and coordinates\n",
    "grid = EclGrid(simcase + \".EGRID\") \n",
    "#init = EclGrid(simcase + \".INIT\") \n",
    "init = EclInitFile(grid, simcase + \".INIT\")\n",
    "# restart file\n",
    "# rst = EclRestartFile(grid, simcase + \".UNRST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The grid dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NX, NY, NZ, total = grid.get_dims()\n",
    "NX, NY, NZ, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store INIT parameters into a Pandas Dataframe: grid_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_init = grid.export_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static properties Dataframe\n",
    "for key in init.keys():\n",
    "        try:\n",
    "                grid_init[key] = init[key][0].numpy_view()\n",
    "        except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cell coordinate X, Y, Z\n",
    "\n",
    "# generate cell coordinates by shifting half cell size\n",
    "xcoord = (grid_init.query(\"j==0&k==0\").DX.cumsum() - grid_init.query(\"j==0&k==0\").DX/2).values\n",
    "ycoord = (grid_init.query(\"i==0&k==0\").DY.cumsum() - grid_init.query(\"i==0&k==0\").DY/2).values\n",
    "zcoord = (grid_init.query(\"i==0&j==0\").DZ.cumsum() - grid_init.query(\"i==0&j==0\").DZ/2).values\n",
    "\n",
    "# TODO(hzh): a bug?\n",
    "# map_X = dict(zip(grid_init.query(\"j==0&j==0\")['i'], xcoord))\n",
    "map_X = dict(zip(grid_init.query(\"j==0&k==0\")['i'], xcoord))\n",
    "map_Y = dict(zip(grid_init.query(\"i==0&k==0\")['j'], ycoord))\n",
    "map_Z = dict(zip(grid_init.query(\"i==0&j==0\")['k'], zcoord))\n",
    "\n",
    "# save cell coordinates to DataFrame\n",
    "grid_init['X'] = grid_init['i'].map(map_X)\n",
    "grid_init['Y'] = grid_init['j'].map(map_Y)\n",
    "grid_init['Z'] = grid_init['k'].map(map_Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot well sketch and 2D slice of the permeability, at coarse grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# middle indices of x and y\n",
    "mid_i = grid_init.i.max()//2\n",
    "mid_j = grid_init.j.max()//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate grid coordinates for plotting\n",
    "\n",
    "# grid coordinates\n",
    "xcorn  = (grid_init.query(\"j==0&k==0\").DX.cumsum()).values\n",
    "ycorn  = (grid_init.query(\"i==0&k==0\").DY.cumsum()).values\n",
    "zcorn  = (grid_init.query(\"i==0&j==0\").DZ.cumsum()).values\n",
    "\n",
    "# add origin coordinates\n",
    "xcorn = np.append(0, xcorn)\n",
    "ycorn = np.append(0, ycorn)\n",
    "zcorn = np.append(0, zcorn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift grid coordinates half-length in x-y directions, i.e., [0, 3900] => [-1900, 2100]\n",
    "# but not in z direction\n",
    "xcorn -= xcoord[mid_i]\n",
    "ycorn -= ycoord[mid_j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 2D xz slice at middle of y\n",
    "XZ_slice = grid_init.query('j==@mid_j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract permeability\n",
    "Z = XZ_slice.PERMX.values.reshape(NZ, NX)\n",
    "\n",
    "# plot x-z slice\n",
    "plot_well_perm(my_well, x=xcorn, y=zcorn, Z=Z, on_coarse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid information for coarse grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve coarse x-y grid indexes where LGR will be placed\n",
    "\n",
    "# TODO(hzh): \n",
    "#   1) why switch i and j? \n",
    "main_grd_i     = grid_init.j.max()//2\n",
    "main_grd_j     = grid_init.i.max()//2\n",
    "\n",
    "# Rettrieve min and max k-index for column where LGR will be placed\n",
    "# TODO(hzh): Do I need to add 1 here?\n",
    "main_grd_min_k = grid_init.k.min()\n",
    "main_grd_max_k = grid_init.k.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve coarse cell sizes\n",
    "main_grd_dx = grid_init.query('i == @main_grd_i & j == @main_grd_j & k == k.min()')['DX'].iloc[0]\n",
    "main_grd_dy = grid_init.query('i == @main_grd_i & j == @main_grd_j & k == k.min()')['DY'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve all DZ in coarse grid, not used\n",
    "main_DZ = grid_init.query('i == @main_grd_i & j == @main_grd_j')['DZ'].values\n",
    "\n",
    "main_DEPTH = grid_init.query('i == @main_grd_i & j == @main_grd_j')['DEPTH'].values\n",
    "\n",
    "# depth where LGR starts\n",
    "ref_depth = 0\n",
    "if Ali_way:\n",
    "    ref_depth = main_DEPTH[0] - 0.5*main_DZ[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve number of cells representing water column and overburden\n",
    "no_of_layers_in_OB    = grid_init.query('i==@main_grd_i & j == @main_grd_j & DZ >  10')['DZ'].shape[0]\n",
    "no_of_layers_below_OB = grid_init.query('i==@main_grd_i & j == @main_grd_j & DZ <= 10')['DZ'].shape[0]\n",
    "\n",
    "no_of_layers_in_OB, no_of_layers_below_OB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes for drilling, casings, borehole and barriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes for the well\n",
    "drilling_df = pd.DataFrame(my_well.drilling)\n",
    "casings_df = pd.DataFrame(my_well.casings)\n",
    "borehole_df = pd.DataFrame(my_well.borehole)\n",
    "\n",
    "# and for the barriers\n",
    "barriers_df = pd.DataFrame(my_well.barriers)\n",
    "barriers_mod_df = pd.DataFrame(my_well.barriers_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute annulus thickness. \n",
    "# For simplicty it assumes annulus between casing and openhole as entire annulus\n",
    "\n",
    "casings_df['ann_od_m'] = np.nan\n",
    "casings_df['ann_bottom_msl'] = np.nan\n",
    "\n",
    "# casing\n",
    "for idx, row in casings_df[::-1].iterrows():\n",
    "\n",
    "        # get id from casing\n",
    "        d, top, bottom = row[['diameter_m', 'top_msl', 'bottom_msl']]\n",
    "\n",
    "        # get od from drilling\n",
    "        hole = drilling_df[drilling_df['diameter_m'] > d].iloc[-1]\n",
    "    \n",
    "        hole_top, hole_bottom, hole_d = hole[['top_msl', 'bottom_msl', 'diameter_m']]\n",
    "\n",
    "        casings_df.loc[idx, 'ann_od_m'] = hole_d       # outer diameter in meters\n",
    "        casings_df.loc[idx, 'ann_bottom_msl'] = hole_bottom\n",
    "\n",
    "#Compute inner area\n",
    "casings_df['A_i'] = np.pi * (casings_df['diameter_m']/2)**2\n",
    "\n",
    "#Compute outer area\n",
    "casings_df['A_o'] = np.pi * (casings_df['ann_od_m']/2)**2\n",
    "\n",
    "# annulus thickness: (od-id)/2\n",
    "casings_df['thick_m'] = (casings_df['ann_od_m'] - casings_df['diameter_m'])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGR grid information in x, y, z directions\n",
    "\n",
    "We are going to compute the grid sizes in lateral (x and y) and vertical directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compute minimum grid size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. minimum grid size\n",
    "\n",
    "# minimum grid size depends on minimum annulus thickness\n",
    "min_grd_size = casings_df['thick_m'].min()\n",
    "\n",
    "if min_grd_size < 0.05:\n",
    "    min_grd_size = 0.05\n",
    "\n",
    "print(f'Minimimum grid size is {min_grd_size*100:.2f} cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(hzh): manually set it\n",
    "if Ali_way:\n",
    "    min_grd_size = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. compute number of LGR grids for drilling, casing and borehole, respectively\n",
    "\n",
    "# for drilling\n",
    "drilling_df['n_grd_id']  = drilling_df['diameter_m'].map(lambda x: compute_ngrd(x, min_grd_size))\n",
    "\n",
    "# the following two are used in ...\n",
    "casings_df[ 'n_grd_id']  = casings_df['diameter_m'].map(lambda x: compute_ngrd(x, min_grd_size))\n",
    "borehole_df['n_grd_id'] = borehole_df['id_m'].map(lambda x: compute_ngrd(x, min_grd_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute LGR grid sizes in x-y directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. compute LGR grid sizes in x-y directions\n",
    "\n",
    "# 2.1 Number of cells representing horizontal LGR\n",
    "no_latral_fine_grd = drilling_df['n_grd_id'].max()\n",
    "no_latral_fine_grd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 generate the LGR grid sizes in x-y\n",
    "LGR_sizes_x, LGR_sizes_y, _ = generate_LGR_xy(no_latral_fine_grd, \n",
    "                                              min_grd_size, \n",
    "                                              main_grd_dx, main_grd_dy,\n",
    "                                              Ali_way=Ali_way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute LGR grid sizes in z direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compute LGR grid sizes in z direction\n",
    "\n",
    "# the dz value to distinguish zones between reservoir and ovb\n",
    "dz0 = 10\n",
    "\n",
    "# 3.1 DZs for reservoir\n",
    "DZ_rsrv = grid_init.query('i==@main_grd_i & j == @main_grd_j & DZ <= @dz0')['DZ'].values\n",
    "\n",
    "# 3.2 DZs for coarse grid\n",
    "DZ_ovb_coarse = grid_init.query('i==@main_grd_i & j == @main_grd_j & DZ > @dz0')['DZ'].values\n",
    "\n",
    "# 3.3 generate the LGR grid sizes in z\n",
    "# LGR_sizes_z, LGR_numb_z, LGR_depths, _ = generate_LGR_z(DZ_rsrv, DZ_ovb_coarse)\n",
    "# TODO(hzh): to make LGR starts at ref_depth\n",
    "LGR_sizes_z, LGR_numb_z, LGR_depths, _ = generate_LGR_z(DZ_rsrv, DZ_ovb_coarse, ref_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGR dimensions\n",
    "nx = len(LGR_sizes_x)\n",
    "ny = len(LGR_sizes_y)\n",
    "nz = len(LGR_sizes_z)\n",
    "\n",
    "nx, ny, nz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up dataframe for LGR mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create i, j, k indices\n",
    "cell_ijk = np.indices((nx, ny, nz))\n",
    "cell_ijk = cell_ijk.reshape((3, nx * ny * nz)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create LGR Dataframe with indices\n",
    "mesh_df = pd.DataFrame(data = cell_ijk, columns = ['i', 'j', 'k'])\n",
    "\n",
    "mesh_df.sort_values(by=['k', 'i'], inplace = True)\n",
    "mesh_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creat DX, DY, DZ for LGR mesh\n",
    "DX_grid, DZ_grid, DY_grid = np.meshgrid(LGR_sizes_x, LGR_sizes_z, LGR_sizes_y)\n",
    "\n",
    "mesh_df['DX'] = DX_grid.flatten()\n",
    "mesh_df['DY'] = DY_grid.flatten()\n",
    "mesh_df['DZ'] = DZ_grid.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cell coordinate X, Y, Z for LGR mesh\n",
    "\n",
    "# cell coordinates\n",
    "xcoord = (mesh_df.query(\"j==0&k==0\").DX.cumsum() - mesh_df.query(\"j==0&k==0\").DX/2).values\n",
    "ycoord = (mesh_df.query(\"i==0&k==0\").DY.cumsum() - mesh_df.query(\"i==0&k==0\").DY/2).values\n",
    "zcoord = (mesh_df.query(\"i==0&j==0\").DZ.cumsum() - mesh_df.query(\"i==0&j==0\").DZ/2).values\n",
    "\n",
    "# TODO(hzh): a bug?\n",
    "# map_X = dict(zip(mesh_df.query(\"j==0&j==0\")['i'], xcoord))\n",
    "map_X = dict(zip(mesh_df.query(\"j==0&k==0\")['i'], xcoord))\n",
    "map_Y = dict(zip(mesh_df.query(\"i==0&k==0\")['j'], ycoord))\n",
    "map_Z = dict(zip(mesh_df.query(\"i==0&j==0\")['k'], zcoord))\n",
    "\n",
    "# save cell coordinates X, Y, Z to dataframe\n",
    "mesh_df['X'] = mesh_df['i'].map(map_X)\n",
    "mesh_df['Y'] = mesh_df['j'].map(map_Y)\n",
    "mesh_df['Z'] = mesh_df['k'].map(map_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save Corner Z points to dataframe\n",
    "mesh_df['Zcorn_top'] = mesh_df['Z'] - mesh_df['DZ']/2\n",
    "mesh_df['Zcorn_bottom'] = mesh_df['Z'] + mesh_df['DZ']/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upscale coarse properties to LGR grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices for center finer grid\n",
    "mid_i = mesh_df.i.max()//2\n",
    "mid_j = mesh_df.j.max()//2\n",
    "mid_i, mid_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# properties\n",
    "fields = ['PORV', \n",
    "          'PERMX', 'PERMY', 'PERMZ', \n",
    "          'MULTX', 'MULTY', 'MULTZ', \n",
    "          'MULTX-', 'MULTY-', 'MULTZ-', \n",
    "          'PORO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upscale coarse properties to LGR grids\n",
    "\n",
    "for field in fields:\n",
    "    # mesh_df[field] = np.nan\n",
    "    mesh_df[field] = 0.0                    # TODO(hzh): what should I put here?\n",
    "    \n",
    "for idx, row in grid_init.query('i==@mid_i & j==@mid_j').iterrows():\n",
    "\n",
    "    # switch to corner coords, coarse grid\n",
    "    top  = row.Z - row.DZ/2\n",
    "    base = row.Z + row.DZ/2\n",
    "\n",
    "    for field in fields:\n",
    "\n",
    "        mesh_df.loc[(mesh_df['Z']>=top) & (mesh_df['Z']<base), field] = row[field]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding box for well elements\n",
    "\n",
    "Compute bounding boxes for drillings, casings and barriers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Drillings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. compute bounding box of drillings\n",
    "\n",
    "drilling_df['k_min'] = np.nan\n",
    "drilling_df['k_max'] = np.nan\n",
    "drilling_df['ij_min'] = np.nan\n",
    "drilling_df['ij_max'] = np.nan\n",
    "\n",
    "for idx, row in drilling_df.iterrows():\n",
    "    \n",
    "    top, bottom = row['top_msl'], row['bottom_msl']\n",
    "\n",
    "    if top < mesh_df['Zcorn_bottom'].max():\n",
    "        \n",
    "        # k ranges\n",
    "        k_min, k_max = get_k_indices(mesh_df, top, bottom)\n",
    "\n",
    "        # x-y ranges\n",
    "        ij_min, ij_max = get_ij_indices(nx, row['n_grd_id'])\n",
    "\n",
    "        # to dataframe\n",
    "        drilling_df.loc[idx, 'k_min'] = k_min\n",
    "        drilling_df.loc[idx, 'k_max'] = k_max\n",
    "        drilling_df.loc[idx, 'ij_min'] = ij_min\n",
    "        drilling_df.loc[idx, 'ij_max'] = ij_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Casings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. computing bounding boxes of casings\n",
    "\n",
    "# casing, k\n",
    "casings_df['k_min'] = np.nan\n",
    "casings_df['k_max'] = np.nan\n",
    "# cement bond, k\n",
    "casings_df['toc_k_min'] = np.nan\n",
    "casings_df['toc_k_max'] = np.nan\n",
    "# casing, xy\n",
    "casings_df['ij_min'] = np.nan\n",
    "casings_df['ij_max'] = np.nan\n",
    "\n",
    "for idx, row in casings_df.iterrows():\n",
    "\n",
    "    # A) casing, z ranges\n",
    "    top, bottom  = row['top_msl'], row['bottom_msl']\n",
    "    \n",
    "    # convert to indices\n",
    "    k_min, k_max = get_k_indices(mesh_df, top, bottom)\n",
    "    \n",
    "    # B) cement, z ranges\n",
    "    toc, boc = row['toc_msl'], row['boc_msl']\n",
    "\n",
    "    # convert to indices\n",
    "    toc_k_min, toc_k_max = get_k_indices(mesh_df, toc, boc)\n",
    "    \n",
    "    # C) xy ranges\n",
    "    ij_min, ij_max = get_ij_indices(nx, row['n_grd_id'])\n",
    "    \n",
    "    # to dataframe\n",
    "    casings_df.loc[idx, 'k_min'] = k_min\n",
    "    casings_df.loc[idx, 'k_max'] = k_max\n",
    "    casings_df.loc[idx, 'toc_k_min'] = toc_k_min\n",
    "    casings_df.loc[idx, 'toc_k_max'] = toc_k_max\n",
    "    casings_df.loc[idx, 'ij_min'] = ij_min\n",
    "    casings_df.loc[idx, 'ij_max'] = ij_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Barriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. computing bounding boxes of barriers\n",
    "\n",
    "barriers_mod_df['k_min'] = np.nan\n",
    "barriers_mod_df['k_max'] = np.nan\n",
    "for idx, row in barriers_mod_df.iterrows():\n",
    "    \n",
    "    k_min, k_max = get_k_indices(mesh_df, row.top_msl, row.bottom_msl)\n",
    "    \n",
    "    barriers_mod_df.loc[idx, 'k_min'] = k_min\n",
    "    barriers_mod_df.loc[idx, 'k_max'] = k_max"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign material types in mesh\n",
    "\n",
    "Assign material types, such as openholes, overburden, cement bond, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize material to overburden\n",
    "mesh_df['material'] = 'overburden'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Drilling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. set material type\n",
    "for idx, row in drilling_df.iterrows():\n",
    "    \n",
    "    top, bottom = row['top_msl'], row['bottom_msl']\n",
    "\n",
    "    if top < mesh_df['Zcorn_bottom'].max():\n",
    "\n",
    "        # extract bounding box\n",
    "        k_min, k_max = row['k_min'], row['k_max']\n",
    "        ij_min, ij_max = row['ij_min'], row['ij_max']\n",
    "        \n",
    "        # 1.1 set material type to openhole\n",
    "        criteria =  '(k >= @k_min) & (k <= @k_max) & (i >= @ij_min) & (i <= @ij_max) & (j >= @ij_min) & (j <= @ij_max)'\n",
    "        mesh_df.loc[mesh_df.eval(criteria), 'material'] = 'openhole'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Casings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. set material types to casings\n",
    "for ic, (idx, row) in enumerate(casings_df.iterrows()):\n",
    "\n",
    "    # extract bounding box\n",
    "    k_min, k_max = row['k_min'], row['k_max']\n",
    "    ij_min, ij_max = row['ij_min'], row['ij_max']\n",
    "    toc_k_min, toc_k_max = row['toc_k_min'], row['toc_k_max']\n",
    "    \n",
    "    # 2.1 set material type to annulus\n",
    "    # x\n",
    "    criteria_i =  '(material == \"openhole\") & (k >= @k_min) & (k <= @k_max) & ((i < @ij_min) | (i > @ij_max))'\n",
    "    mesh_df.loc[mesh_df.eval(criteria_i), 'material'] = 'annulus'\n",
    "    # y\n",
    "    criteria_j =  '(material == \"openhole\") & (k >= @k_min) & (k <= @k_max) & ((j < @ij_min) | (j > @ij_max))'\n",
    "    mesh_df.loc[mesh_df.eval(criteria_j), 'material'] = 'annulus'\n",
    "    \n",
    "    # 2.2 set material type to cement_bond\n",
    "    criteria = '(material == \"annulus\") & (k >= @toc_k_min) & (k <= @toc_k_max)' \n",
    "    mesh_df.loc[mesh_df.eval(criteria), 'material'] = f'cement_bond_{ic}'\n",
    "\n",
    "    # 2.3 set material type to openhole\n",
    "    criteria = '(material == \"annulus\")'  \n",
    "    mesh_df.loc[mesh_df.eval(criteria), 'material'] = 'openhole'\n",
    "    # mesh_df.loc[mesh_df.eval(criteria_j), 'material'] = 'cementbond'   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Barriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. set material type to barrier\n",
    "for ib, (idx, row) in enumerate(barriers_mod_df.iterrows()):\n",
    "    \n",
    "    b_k_min, b_k_max = row['k_min'], row['k_max']\n",
    "    \n",
    "    criteria = '(material == \"openhole\") & (k >= @b_k_min) & (k <= @b_k_max)' \n",
    "    mesh_df.loc[mesh_df.eval(criteria), 'material'] = f'barrier_{ib}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign permeability according to material type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barriers_mod_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set permeability according to material type\n",
    "\n",
    "# 1. openhole\n",
    "oh_perm = drilling_df['oh_perm'].iloc[0]\n",
    "criteria = 'material == \"openhole\"'\n",
    "mesh_df.loc[mesh_df.eval(criteria), 'PERMX'] = oh_perm\n",
    "\n",
    "# 2. cement bond\n",
    "for ic, (_, row) in enumerate(casings_df.iterrows()):\n",
    "    cb_perm = row['cb_perm']\n",
    "    criteria = f'material == \"cement_bond_{ic}\"'\n",
    "    mesh_df.loc[mesh_df.eval(criteria), 'PERMX'] = cb_perm\n",
    "\n",
    "# 3. barrier\n",
    "for ib, (_, row) in enumerate(barriers_mod_df.iterrows()):\n",
    "    barrier_perm = row['barrier_perm']\n",
    "    criteria = f'material == \"barrier_{ib}\"'\n",
    "    mesh_df.loc[mesh_df.eval(criteria), 'PERMX'] = barrier_perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of well sketch and LGR grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid coordinates at LGR grids for plotting\n",
    "xcorn  = (mesh_df.query(\"j==0&k==0\").DX.cumsum()).values\n",
    "ycorn  = (mesh_df.query(\"i==0&k==0\").DY.cumsum()).values\n",
    "zcorn  = (mesh_df.query(\"i==0&j==0\").DZ.cumsum()).values\n",
    "\n",
    "# add origin\n",
    "xcorn = np.append(0, xcorn)\n",
    "ycorn = np.append(0, ycorn)\n",
    "zcorn = np.append(0, zcorn)\n",
    "\n",
    "# shift it\n",
    "xcorn -= main_grd_dx/2\n",
    "ycorn -= main_grd_dy/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LGR\n",
    "XZ_slice = mesh_df.query('j==@mid_i')\n",
    "\n",
    "# extract permeability value\n",
    "Z = XZ_slice.PERMX.values.reshape(nz, nx)\n",
    "\n",
    "# plot it\n",
    "plot_well_perm(my_well, x=xcorn, y=zcorn, Z=Z, on_coarse=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write LGR file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRG name \n",
    "LGR_NAME = 'LEG_HIRES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare info about Casing, Cement Bond and Open hole  for GaP\n",
    "casing_list = to_gap_casing_list(drilling_df, \n",
    "                              casings_df)\n",
    "\n",
    "# prepare info about Barrier for GaP \n",
    "barrier_list = to_gap_barrier_list(barriers_mod_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate .grdecl file\n",
    "# TODO(hzh): add 1s to indices here\n",
    "build_grdecl(output_dir, LGR_NAME,\n",
    "                casing_list,\n",
    "                barrier_list,\n",
    "                LGR_sizes_x, \n",
    "                LGR_depths, \n",
    "                LGR_numb_z, \n",
    "                min_grd_size,\n",
    "                grid.getNX(), grid.getNY(),\n",
    "                main_grd_i + 1, main_grd_j + 1,\n",
    "                main_grd_min_k + 1, main_grd_max_k + 1,\n",
    "                no_of_layers_in_OB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l LEG_HIRES.grdecl*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results, only compare smeaheia_v1\n",
    "if case_type == 'smeaheia_v1' and Ali_way:\n",
    "    !diff LEG_HIRES.grdecl LEG_HIRES.grdecl.smeaheia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
